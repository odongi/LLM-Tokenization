{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1G9Pd6NcorNaA4gbZvqyrvk4s8-WGHnWz","authorship_tag":"ABX9TyOYyuQ16oxgJA6t74CW/RtU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"iIfL_VY_tmi7","executionInfo":{"status":"ok","timestamp":1764922530141,"user_tz":-180,"elapsed":14,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"outputs":[],"source":["#CREATING TOKENS FOR LLM"]},{"cell_type":"code","source":["#Importing our dataset\n","with open(\"/content/drive/MyDrive/TOKENIZATION/the-verdict.txt\",\"r\", encoding=\"UTF-8\") as f:\n","  raw_text = f.read()\n","print(\"Total numbers of characters:\",len(raw_text))\n","print(raw_text[:99])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ3N5nqht1vS","executionInfo":{"status":"ok","timestamp":1764922531708,"user_tz":-180,"elapsed":1549,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"65807a83-2ad9-4473-da15-165edf2c1ff0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Total numbers of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}]},{"cell_type":"code","source":["#The goal is to tokenize these 20479 characters."],"metadata":{"id":"QppH6nvwv3TF","executionInfo":{"status":"ok","timestamp":1764922531869,"user_tz":-180,"elapsed":144,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#I use the \"re\" library for spliting into token/individual words.\n","#Example:\n","import re\n","text = \"Hello, this is Glen. Be free.\"\n","result = re.split(r'(\\s)',text) #Here the '(\\s)' splits where there is a white space.\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5akrfOfhw5Ab","executionInfo":{"status":"ok","timestamp":1764922531871,"user_tz":-180,"elapsed":117,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"145e64c9-d1f8-4a13-f0ae-3269f37a5759"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'this', ' ', 'is', ' ', 'Glen.', ' ', 'Be', ' ', 'free.']\n"]}]},{"cell_type":"code","source":["#But I want commas and fullstops to be split individually therefore:\n","result = re.split(r'([,.]|\\s)',text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAQrhrcFxoEi","executionInfo":{"status":"ok","timestamp":1764922531871,"user_tz":-180,"elapsed":27,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"c5c0d299-44cc-403b-b32c-efbb3b06cc13"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'this', ' ', 'is', ' ', 'Glen', '.', '', ' ', 'Be', ' ', 'free', '.', '']\n"]}]},{"cell_type":"code","source":["#But the white space characters are very much available and they're not required therefore:\n","result = [item for item in result if item.strip()] #.strip() removes whitespace.\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmhzrDjmye0Y","executionInfo":{"status":"ok","timestamp":1764922531873,"user_tz":-180,"elapsed":20,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"5efe3a01-a281-42cf-87ee-caac295508be"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'this', 'is', 'Glen', '.', 'Be', 'free', '.']\n"]}]},{"cell_type":"code","source":["#Now watch me apply this tokenization scheme to the raw_text.\n","preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n","preprocessed = [item for item in preprocessed if item.strip()]\n","print(preprocessed[:30]) #Just checking the first 30 tokens."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVLu2qQfzF6F","executionInfo":{"status":"ok","timestamp":1764922531927,"user_tz":-180,"elapsed":41,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"3a473abc-f2bb-4b15-eaa6-ed739854e017"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"code","source":["print(len(preprocessed))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XlBJKtEv1mEi","executionInfo":{"status":"ok","timestamp":1764922531950,"user_tz":-180,"elapsed":19,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"d507e980-f14f-4c85-9ed8-bcadb32f56eb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["4690\n"]}]},{"cell_type":"code","source":["#Create token IDs\n","all_words = sorted(set(preprocessed)) #Sorts in alphabetical order\n","vocabulary_size = len(all_words)\n","print(vocabulary_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNqm2suG1zRx","executionInfo":{"status":"ok","timestamp":1764922532118,"user_tz":-180,"elapsed":165,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"c76aa193-5e02-43e4-9b9f-03ffe34df99c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["1130\n"]}]},{"cell_type":"code","source":["#Now the IDs\n","vocab =  {token:integer for integer, token in enumerate(all_words)} #Assigning each unique token with a unique number ID.\n","for i, item in enumerate(vocab.items()):\n","  print(item)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCGXF24W3DbW","executionInfo":{"status":"ok","timestamp":1764922532237,"user_tz":-180,"elapsed":117,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"ad9aa22f-2bc6-4d02-ded2-92a63920d586"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"code","source":["#Instantiate a class for tokenization\n","class SimpleTokenizerV1:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()} #'s' is our token 'i' is the unique id for vocab.\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s',text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","    #Replacing the spaces before the punctuations\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"Lv255O9N4Sbi","executionInfo":{"status":"ok","timestamp":1764922532242,"user_tz":-180,"elapsed":3,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#Let's test the class above.\n","tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qote2K_6-Way","executionInfo":{"status":"ok","timestamp":1764922532257,"user_tz":-180,"elapsed":12,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"f9fc2414-9442-4f87-873b-19c0e02a3241"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[56, 850, 988, 602, 533, 746, 1126, 596, 67, 38, 851, 1108, 754, 793]\n"]}]},{"cell_type":"code","source":["#Now let's see if it can convert the token ids into text\n","tokenizer.decode(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qJ98vySy_muk","executionInfo":{"status":"ok","timestamp":1764922532327,"user_tz":-180,"elapsed":53,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"cfccd14c-210b-4898-fca6-ce7d22828a39"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'It s the last he painted you know Mrs Gisburn said with pardonable pride'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# #Let's try to use words not in the vocabulary.\n","# text = \"Glen, do you like tea\"\n","# tokenizer.encode(text)\n","# #Of course there's a key error i.e Glen. This highlights the need to consider large and diverse training sets to extend vocabulary."],"metadata":{"id":"oFfo-isoAc09","executionInfo":{"status":"ok","timestamp":1764922532371,"user_tz":-180,"elapsed":6,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#Adding Special Context Tokens to handle unknown words.\n","#I can modify the tokenizer to add a new unknow token |unk| that handles unknown words and |endoftext| token for between unrelated text.\n","all_tokens = sorted(set(preprocessed))\n","all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n","vocab = {token:integer for integer, token in enumerate(all_tokens)}"],"metadata":{"id":"BiFcVrIWBK4s","executionInfo":{"status":"ok","timestamp":1764922532386,"user_tz":-180,"elapsed":8,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["len(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seibE9tUEuqj","executionInfo":{"status":"ok","timestamp":1764922532413,"user_tz":-180,"elapsed":23,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"9d34d851-4545-481f-bfd7-233cd6d6e0fb"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1132"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#Let's see the last 5 entries in the updated vocab.\n","for i, item in enumerate(list(vocab.items())[-5:]):\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35JK1SnqEwIX","executionInfo":{"status":"ok","timestamp":1764922532436,"user_tz":-180,"elapsed":20,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"9045dada-8939-4df5-ec70-ef11dfa5d3a9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"code","source":[" #Instantiate a class for tokenization\n","class SimpleTokenizerV2:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()} #'s' is our token 'i' is the unique id for vocab.\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n","    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","    preprocessed = [\n","        item if item in self.str_to_int\n","        else \"<|unk|>\" for item in preprocessed #Dealing with unkown values.\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[s] for s in ids])\n","    #Replacing the spaces before the punctuations\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"vhEwyRqfFhB1","executionInfo":{"status":"ok","timestamp":1764922532460,"user_tz":-180,"elapsed":22,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#Now let's see what V2 can do.\n","tokenizer = SimpleTokenizerV2(vocab)"],"metadata":{"id":"zqNleSQLHK3a","executionInfo":{"status":"ok","timestamp":1764922532530,"user_tz":-180,"elapsed":50,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#Our unknowns are \"Glen\" and \"palace\".\n","text1 = \"Glen, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1,text2))\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaET2VEZHZk5","executionInfo":{"status":"ok","timestamp":1764922532554,"user_tz":-180,"elapsed":11,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"1fe67b8c-6663-461e-c173-c067b9e72edd"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Glen, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["tokenizer.encode(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqCAJt9AH5G6","executionInfo":{"status":"ok","timestamp":1764922532563,"user_tz":-180,"elapsed":7,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"a6169c43-749d-4b2c-eb9b-ff36609099b7"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1131, 355, 1126, 628, 975, 1130, 55, 988, 956, 984, 722, 988, 1131]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["tokenizer.decode(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"THf8goPWLcNl","executionInfo":{"status":"ok","timestamp":1764922532570,"user_tz":-180,"elapsed":5,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"26faac23-aaf8-41f9-a8bb-348e712263cb"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|unk|> do you like tea <|endoftext|> In the sunlit terraces of the <|unk|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["#There are other 3 special tokens i.e:\n","# 1: beginning of sequence(BOS) that signifies to the LLM where a piece of content begins.\n","# 2: end of sequence(EOS) which is useful in concatenating multiple unrelated texts.\n","# 3: padding (PAD) which ensures all texts have tehe same length.\n","#GPT models don't rely on these three tokens except for <|endoftext|>. They also don't use <|unk|> for unknowwn words instead they break the words into subwords units through Byte Pair Encoding."],"metadata":{"id":"6u6FcV_VMWFr","executionInfo":{"status":"ok","timestamp":1764922532614,"user_tz":-180,"elapsed":8,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#BPE IS MUCH MORE EFFECTIVE IN TOKENIZATION THUS IT SHALL BE IMPLEMENTED.\n","! pip3 install tiktoken #tiktoken is BPE tokenizer made for OpenAi models. You can check it on github."],"metadata":{"id":"aietucscMz7-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764925947927,"user_tz":-180,"elapsed":5341,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"ef2707f3-e473-4484-e343-a3f5af25e654"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"]}]},{"cell_type":"code","source":["import importlib\n","import tiktoken\n","print(\"Tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fw4hh-myKkQ","executionInfo":{"status":"ok","timestamp":1764926238446,"user_tz":-180,"elapsed":1643,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"539f8d17-69f5-45f3-8655-f7a5810a3445"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiktoken version: 0.12.0\n"]}]},{"cell_type":"code","source":["#Now instantiating the tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"IZOW7FVbzNkr","executionInfo":{"status":"ok","timestamp":1764926325270,"user_tz":-180,"elapsed":5370,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["#Now let's see if it will produce some ids.\n","text = (\n","    \"Glen, do you like tea? <|endoftext|> In the sunlit terraces of\"\n","    \"of someunknownPlace.\"\n",")\n","integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QrqzGRKzmrC","executionInfo":{"status":"ok","timestamp":1764926650332,"user_tz":-180,"elapsed":18,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"6968a503-127b-464e-fcab-da509cf6ad7b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[9861, 268, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["#Converting the ids back to text.\n","our_text = tokenizer.decode(integers)\n","print(our_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dS5g55D503Ta","executionInfo":{"status":"ok","timestamp":1764926850154,"user_tz":-180,"elapsed":12,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"d4aac75c-391e-4474-9f9c-499a8c7defb2"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Glen, do you like tea? <|endoftext|> In the sunlit terraces ofof someunknownPlace.\n"]}]},{"cell_type":"code","source":["#Let's just use random words and try to encode and decode it.\n","text = \"Dgdneh hdkba\"\n","integers = tokenizer.encode(text)\n","print(integers)\n","string = tokenizer.decode(integers)\n","print(string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EuO1SMA-1kiO","executionInfo":{"status":"ok","timestamp":1764927183671,"user_tz":-180,"elapsed":29,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"17450f30-ba75-4a51-af53-a414e3d1141f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[35, 21287, 710, 71, 289, 34388, 7012]\n","Dgdneh hdkba\n"]}]},{"cell_type":"code","source":["#CREATING INPUT-TARGET PAIRS"],"metadata":{"id":"jXLFr2Hf2t94","executionInfo":{"status":"ok","timestamp":1764928874588,"user_tz":-180,"elapsed":48,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["#First encode the dataset using BPE\n","with open(\"/content/drive/MyDrive/TOKENIZATION/the-verdict.txt\",\"r\",encoding=\"UTF-8\") as f:\n","  raw_text = f.read()\n","\n","encoded_text = tokenizer.encode(raw_text)\n","print(len(encoded_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0RJzCKp9WX_","executionInfo":{"status":"ok","timestamp":1764929050166,"user_tz":-180,"elapsed":27,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"a6cebbbd-a770-4c28-9d38-44c1d011b5b6"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["5145\n"]}]},{"cell_type":"code","source":["#So the vocabulary size is 5145."],"metadata":{"id":"DsMMglIH-BNi","executionInfo":{"status":"ok","timestamp":1764929099945,"user_tz":-180,"elapsed":56,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["#Let's see the remove the first 50 tokens\n","encoded_sample = encoded_text[50:]"],"metadata":{"id":"ABBFD22s-NZK","executionInfo":{"status":"ok","timestamp":1764929466906,"user_tz":-180,"elapsed":77,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["#The context size determines how many tokens are included in the output\n","context_size = 7 #So the model will look at a sequence of 7 words to to predict the next word in the sequence.\n","#For example: input x has the first 4 tokens [1,2,3,4] and the target y is the next 4 tokens [2,3,4,5]\n","\n","x = encoded_sample[:context_size] #Take the first 7.\n","y = encoded_sample[1:context_size+1] #Shifting the input by 1 position.\n","print(f\"x:{x}\")\n","print(f\"y:     {y}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BItPi9vs_m-_","executionInfo":{"status":"ok","timestamp":1764929901357,"user_tz":-180,"elapsed":9,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"c5ff91d7-dddf-4fe7-bc74-51a4cf766b81"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["x:[290, 4920, 2241, 287, 257, 4489, 64]\n","y:     [4920, 2241, 287, 257, 4489, 64, 319]\n"]}]},{"cell_type":"code","source":["#In a for loop it'll look like:\n","for i in range(1, context_size+1): #This loop will go from 1 to 7\n","  context = encoded_sample[:i]\n","  desired = encoded_sample[i]\n","  print(context,\"--------->\", desired)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykPXXEQ2BIHF","executionInfo":{"status":"ok","timestamp":1764930718387,"user_tz":-180,"elapsed":9,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"90fd90b9-630e-4a59-b862-6a976cd85033"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[290] ---------> 4920\n","[290, 4920] ---------> 2241\n","[290, 4920, 2241] ---------> 287\n","[290, 4920, 2241, 287] ---------> 257\n","[290, 4920, 2241, 287, 257] ---------> 4489\n","[290, 4920, 2241, 287, 257, 4489] ---------> 64\n","[290, 4920, 2241, 287, 257, 4489, 64] ---------> 319\n"]}]},{"cell_type":"code","source":["#In short, everything left of \"--------->\" is the input an LLM would receive and the on the right is what it would try to predict."],"metadata":{"id":"XiJ2rXaSEBTU","executionInfo":{"status":"ok","timestamp":1764930910644,"user_tz":-180,"elapsed":5,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["#Now let's turn it into text to get an overall understanding.\n","for i in range(1, context_size+1): #This loop will go from 1 to 7\n","  context = encoded_sample[:i]\n","  desired = encoded_sample[i]\n","  print(tokenizer.decode(context),\"--------->\", tokenizer.decode([desired]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcbVJE2vFHeQ","executionInfo":{"status":"ok","timestamp":1764931054835,"user_tz":-180,"elapsed":25,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"94584a98-6c9c-49df-b709-00f4defda9bc"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":[" and --------->  established\n"," and established --------->  himself\n"," and established himself --------->  in\n"," and established himself in --------->  a\n"," and established himself in a --------->  vill\n"," and established himself in a vill ---------> a\n"," and established himself in a villa --------->  on\n"]}]},{"cell_type":"code","source":["#Now the inputs are required to be in tensors (input tensor for the LLM and target tensor for the LLM to predict) hence the introduction of a DATALOADER."],"metadata":{"id":"F3n88W44Fi41","executionInfo":{"status":"ok","timestamp":1764931298704,"user_tz":-180,"elapsed":52,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class GPTDatasetV1(Dataset):\n","  def __init__(self, txt, tokenizer, max_length, stride): #max_length is the context size.  stride so as to know how much to slide.\n","    self.input_ids = []\n","    self.target_ids = []\n","\n","    #Tokenize the entire text.\n","    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","    #Using a sliding window to chunk the book (The verdict) into overlapping sequence of max_length\n","    for i in range(0, len(token_ids) - max_length, stride): #I don't want to spill over the dataset hence '- max_length'.\n","      input_chunk = token_ids[i:i + max_length]\n","      target_chunk = token_ids[i:i + max_length + 1]\n","      self.input_ids.append(torch.tensor(input_chunk))\n","      self.target_ids.append(torch.tensor(target_chunk))\n","\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"uVjQVjJLGmL0","executionInfo":{"status":"ok","timestamp":1764934985481,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0): #drop_last=True so as to prevent loss spikes i.e if the last batch is not the same as the specified batch_size.\n","\n","  #Initialize the tokenizer.\n","  tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","  #Create Dataset\n","  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","  #Create Dataloader\n","  dataloader = DataLoader(\n","      dataset,\n","      batch_size=batch_size,\n","      shuffle=shuffle,\n","      drop_last=drop_last,\n","      num_workers=num_workers\n","  )\n","\n","  return dataloader"],"metadata":{"id":"yADMT-S7PItJ","executionInfo":{"status":"ok","timestamp":1764934986725,"user_tz":-180,"elapsed":44,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["#Testing the dataloader with a batch size of 1 with context size of 4.\n","with open(\"/content/drive/MyDrive/TOKENIZATION/the-verdict.txt\", \"r\", encoding=\"UTF-8\") as f:\n","  raw_text = f.read()"],"metadata":{"id":"ug-WxEaPR2kb","executionInfo":{"status":"ok","timestamp":1764934988410,"user_tz":-180,"elapsed":7,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["#Next converting the dataloader into a python iterator to fetch the next entry via python's built-in next functionI()\n","import torch\n","print(\"Pytorch version:\", torch.__version__)\n","dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24AATe7AS5oA","executionInfo":{"status":"ok","timestamp":1764934989663,"user_tz":-180,"elapsed":161,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"3a635b81-0eef-4e66-9b5a-14594e111c5f"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version: 2.9.0+cu126\n","[tensor([[  40,  367, 2885, 1464]]), tensor([[  40,  367, 2885, 1464, 1807]])]\n"]}]},{"cell_type":"code","source":["second_batch = next(data_iter)\n","print(second_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJgi2QyeUIN9","executionInfo":{"status":"ok","timestamp":1764935220861,"user_tz":-180,"elapsed":29,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"e854de91-de46-4555-effc-3f6ffc8ed7d2"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[ 367, 2885, 1464, 1807]]), tensor([[ 367, 2885, 1464, 1807, 3619]])]\n"]}]},{"cell_type":"code","source":["#Looking at first_batcch and second_batch we can see the stride is one.\n","#Smaller batches save computational power but are more noisy during training."],"metadata":{"id":"WumflPHbVjuc","executionInfo":{"status":"ok","timestamp":1764935481220,"user_tz":-180,"elapsed":41,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["#Let's see the result when the batch_size is greater than 1.\n","dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\",inputs)\n","print(\"Targets:\\n\",targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hI2TYDN-WjUp","executionInfo":{"status":"ok","timestamp":1764935669613,"user_tz":-180,"elapsed":59,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"2196087b-0db1-49a0-ebf0-3019c4c2cd69"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputs:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","Targets:\n"," tensor([[   40,   367,  2885,  1464,  1807],\n","        [ 1807,  3619,   402,   271, 10899],\n","        [10899,  2138,   257,  7026, 15632],\n","        [15632,   438,  2016,   257,   922],\n","        [  922,  5891,  1576,   438,   568],\n","        [  568,   340,   373,   645,  1049],\n","        [ 1049,  5975,   284,   502,   284],\n","        [  284,  3285,   326,    11,   287]])\n"]}]},{"cell_type":"code","source":["#The increase in batch size is to prevent overfitting and to ensure we don't skip a single word and avoid any overlapping."],"metadata":{"id":"eJ7L6i1WXRR7"},"execution_count":null,"outputs":[]}]}
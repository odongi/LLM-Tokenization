{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1G9Pd6NcorNaA4gbZvqyrvk4s8-WGHnWz","authorship_tag":"ABX9TyNPxyQiKDGwMWFbzJaCd0w+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"id":"iIfL_VY_tmi7","executionInfo":{"status":"ok","timestamp":1764845346834,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"outputs":[],"source":["#CREATING TOKENS FOR LLM"]},{"cell_type":"code","source":["#Importing our dataset\n","with open(\"/content/drive/MyDrive/TOKENIZATION/the-verdict.txt\",\"r\", encoding=\"UTF-8\") as f:\n","  raw_text = f.read()\n","print(\"Total numbers of characters:\",len(raw_text))\n","print(raw_text[:99])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ3N5nqht1vS","executionInfo":{"status":"ok","timestamp":1764841453257,"user_tz":-180,"elapsed":189,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"ca927478-d6f9-4439-ab05-decfde3679ec"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Total numbers of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}]},{"cell_type":"code","source":["#The goal is to tokenize these 20479 characters."],"metadata":{"id":"QppH6nvwv3TF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#I use the \"re\" library for spliting into token/individual words.\n","#Example:\n","import re\n","text = \"Hello, this is Glen. Be free.\"\n","result = re.split(r'(\\s)',text) #Here the '(\\s)' splits where there is a white space.\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5akrfOfhw5Ab","executionInfo":{"status":"ok","timestamp":1764841915009,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"4cb2cea2-b04e-417d-b1a3-18e7ce0dff52"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'this', ' ', 'is', ' ', 'Glen.', ' ', 'Be', ' ', 'free.']\n"]}]},{"cell_type":"code","source":["#But I want commas and fullstops to be split individually therefore:\n","result = re.split(r'([,.]|\\s)',text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAQrhrcFxoEi","executionInfo":{"status":"ok","timestamp":1764842139249,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"8350315c-ee6f-4407-ef4a-fb5e81a0c164"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'this', ' ', 'is', ' ', 'Glen', '.', '', ' ', 'Be', ' ', 'free', '.', '']\n"]}]},{"cell_type":"code","source":["#But the white space characters are very much available and they're not required therefore:\n","result = [item for item in result if item.strip()] #.strip() removes whitespace.\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmhzrDjmye0Y","executionInfo":{"status":"ok","timestamp":1764842299374,"user_tz":-180,"elapsed":20,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"96f83592-f629-400d-aea9-d3d98188ca09"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'this', 'is', 'Glen', '.', 'Be', 'free', '.']\n"]}]},{"cell_type":"code","source":["#Now watch me apply this tokenization scheme to the raw_text.\n","preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n","preprocessed = [item for item in preprocessed if item.strip()]\n","print(preprocessed[:30]) #Just checking the first 30 tokens."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVLu2qQfzF6F","executionInfo":{"status":"ok","timestamp":1764848787731,"user_tz":-180,"elapsed":48,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"021e5a89-be9e-4b74-807a-a6076755aa29"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"code","source":["print(len(preprocessed))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XlBJKtEv1mEi","executionInfo":{"status":"ok","timestamp":1764848793325,"user_tz":-180,"elapsed":6,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"28ece8b7-a377-4cb1-e021-eecd0c9b4906"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["4690\n"]}]},{"cell_type":"code","source":["#Create token IDs\n","all_words = sorted(set(preprocessed)) #Sorts in alphabetical order\n","vocabulary_size = len(all_words)\n","print(vocabulary_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HNqm2suG1zRx","executionInfo":{"status":"ok","timestamp":1764848794178,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"59c9e5f0-8400-4839-8926-345156543b70"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["1130\n"]}]},{"cell_type":"code","source":["#Now the IDs\n","vocab =  {token:integer for integer, token in enumerate(all_words)} #Assigning each unique token with a unique number ID.\n","for i, item in enumerate(vocab.items()):\n","  print(item)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCGXF24W3DbW","executionInfo":{"status":"ok","timestamp":1764848795926,"user_tz":-180,"elapsed":9,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"396c1b20-8521-442b-fbba-1d6897e2c826"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"code","source":["#Instantiate a class for tokenization\n","class SimpleTokenizerV1:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()} #'s' is our token 'i' is the unique id for vocab.\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s',text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","    #Replacing the spaces before the punctuations\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"Lv255O9N4Sbi","executionInfo":{"status":"ok","timestamp":1764848796944,"user_tz":-180,"elapsed":17,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["#Let's test the class above.\n","tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qote2K_6-Way","executionInfo":{"status":"ok","timestamp":1764848798041,"user_tz":-180,"elapsed":19,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"ab6c4c49-c77d-4964-85ee-aea6db3281bc"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["[56, 850, 988, 602, 533, 746, 1126, 596, 67, 38, 851, 1108, 754, 793]\n"]}]},{"cell_type":"code","source":["#Now let's see if it can convert the token ids into text\n","tokenizer.decode(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qJ98vySy_muk","executionInfo":{"status":"ok","timestamp":1764848798930,"user_tz":-180,"elapsed":35,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"b90fe4c5-859a-4c68-9dc4-b0058d188ebb"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'It s the last he painted you know Mrs Gisburn said with pardonable pride'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["# #Let's try to use words not in the vocabulary.\n","# text = \"Glen, do you like tea\"\n","# tokenizer.encode(text)\n","# #Of course there's a key error i.e Glen. This highlights the need to consider large and diverse training sets to extend vocabulary."],"metadata":{"id":"oFfo-isoAc09","executionInfo":{"status":"ok","timestamp":1764848799753,"user_tz":-180,"elapsed":26,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["#Adding Special Context Tokens to handle unknown words.\n","#I can modify the tokenizer to add a new unknow token |unk| that handles unknown words and |endoftext| token for between unrelated text.\n","all_tokens = sorted(set(preprocessed))\n","all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n","vocab = {token:integer for integer, token in enumerate(all_tokens)}"],"metadata":{"id":"BiFcVrIWBK4s","executionInfo":{"status":"ok","timestamp":1764848800259,"user_tz":-180,"elapsed":12,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["len(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seibE9tUEuqj","executionInfo":{"status":"ok","timestamp":1764848800715,"user_tz":-180,"elapsed":64,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"d93d4f42-077b-42a0-b33f-6af818bb951c"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1132"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["#Let's see the last 5 entries in the updated vocab.\n","for i, item in enumerate(list(vocab.items())[-5:]):\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35JK1SnqEwIX","executionInfo":{"status":"ok","timestamp":1764848801599,"user_tz":-180,"elapsed":250,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"d03ca520-9ae4-4e41-e82c-b51d3c4d44b8"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"code","source":[" #Instantiate a class for tokenization\n","class SimpleTokenizerV2:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()} #'s' is our token 'i' is the unique id for vocab.\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'[,.:;?_!\"()\\']|--|\\s', text)\n","    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","    preprocessed = [\n","        item if item in self.str_to_int\n","        else \"<|unk|>\" for item in preprocessed #Dealing with unkown values.\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[s] for s in ids])\n","    #Replacing the spaces before the punctuations\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"vhEwyRqfFhB1","executionInfo":{"status":"ok","timestamp":1764848801990,"user_tz":-180,"elapsed":6,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["#Now let's see what V2 can do.\n","tokenizer = SimpleTokenizerV2(vocab)"],"metadata":{"id":"zqNleSQLHK3a","executionInfo":{"status":"ok","timestamp":1764848802742,"user_tz":-180,"elapsed":39,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["#Our unknowns are \"Glen\" and \"palace\".\n","text1 = \"Glen, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1,text2))\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaET2VEZHZk5","executionInfo":{"status":"ok","timestamp":1764848803379,"user_tz":-180,"elapsed":21,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"e8eb23b0-a50b-4971-8a93-f42f290d30f7"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Glen, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["tokenizer.encode(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqCAJt9AH5G6","executionInfo":{"status":"ok","timestamp":1764848805490,"user_tz":-180,"elapsed":22,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"72ac9d1e-6d3e-4478-e2e5-ec3f01701ab5"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1131, 355, 1126, 628, 975, 1130, 55, 988, 956, 984, 722, 988, 1131]"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["tokenizer.decode(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"THf8goPWLcNl","executionInfo":{"status":"ok","timestamp":1764848919257,"user_tz":-180,"elapsed":43,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}},"outputId":"4eaccc05-94bb-4b85-ccd4-d3c18ceeb51a"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|unk|> do you like tea <|endoftext|> In the sunlit terraces of the <|unk|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["#There are other 3 special tokens i.e:\n","# 1: beginning of sequence(BOS) that signifies to the LLM where a piece of content begins.\n","# 2: end of sequence(EOS) which is useful in concatenating multiple unrelated texts.\n","# 3: padding (PAD) which ensures all texts have tehe same length.\n","#GPT models don't rely on these three tokens except for <|endoftext|>. They also don't use <|unk|> for unknowwn words instead they break the words into subwords units through Byte Pair Encoding."],"metadata":{"id":"6u6FcV_VMWFr","executionInfo":{"status":"ok","timestamp":1764849292727,"user_tz":-180,"elapsed":47,"user":{"displayName":"Glen Ouma","userId":"01991645082034555818"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aietucscMz7-"},"execution_count":null,"outputs":[]}]}